{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multinomial Naive Bayes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMzh4Bsa3FV6ELnbFa2QptR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joereuben/language-identification/blob/main/Multinomial_Naive_Bayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The Objective**\n",
        "I am tasked with writing a language identification program that can detect the language of text given to it. The output of the prediction will be the language code.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "I will be using character bigrams for vectorization and the naive bayes algorithm as the classifier"
      ],
      "metadata": {
        "id": "NILaxn8wXeoB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Importing Libraries\n",
        "IN this program, I will be using four txt files containing text in 4 different languages. The texts will have to be cleaned, vectorized before being trained by the algorithm"
      ],
      "metadata": {
        "id": "OVxxJmWYY7ml"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "pmqg81kZKrHv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics import confusion_matrix,classification_report"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing Function\n",
        "A function for cleaning up our data by removing non-alphabetic characters before feeding it into the algorithm"
      ],
      "metadata": {
        "id": "bBNjsMFrcY7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_data(file):\n",
        "   \n",
        "   data = \"\"\n",
        "   with open(file,\"r\",encoding=\"utf-8\") as f:\n",
        "      data = f.read()\n",
        " \n",
        "   data = data.replace(\"?\",\".\")\n",
        "   data = data.replace(\"!\",\".\")\n",
        "   data = data.replace(\"»\",\"\")\n",
        "   data = data.replace(\"«\",\"\")\n",
        "   data = data.replace(\":\",\"\")\n",
        "   data = data.replace(\";\",\"\")\n",
        "   data = data.replace(\"...\",\".\")\n",
        "   data = data.replace(\"…\",\".\")\n",
        "   data = data.replace(\"\\n\",\".\")\n",
        "   data = data.replace(\"  \",\" \")\n",
        "   data = data.replace(\"\\\"\",\"\")\n",
        "   data = data.replace(\"„\",\"\")\n",
        "   sentences = data.split(\".\")\n",
        "   for i in range(len(sentences)):\n",
        "      sentences[i] = sentences[i].strip()\n",
        "      \n",
        "   sentences = [x for x in sentences if x != \"\"]\n",
        "   return sentences"
      ],
      "metadata": {
        "id": "mJyDzeIZLF3C"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Reading\n",
        "Loading the txt files that contains our sample data. These will be cleaned by the function above, used to create an array of the languages and then another array with the language code related to each language "
      ],
      "metadata": {
        "id": "zA_QVNgqc3b6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "italian = process_data(\"il fratello maggiore sta guardando.txt\")\n",
        "english = process_data(\"big brother is watching.txt\")\n",
        "german = process_data(\"großer bruder schaut zu.txt\")\n",
        "portuguese = process_data(\"irmão mais velho está assistindo.txt\")\n",
        "\n",
        "X = np.array(italian + english + german + portuguese)\n",
        "y = np.array(['it']*len(italian) + ['en']*len(english) + ['de']*len(german) + ['pt']*len(portuguese))\n"
      ],
      "metadata": {
        "id": "NYF_58jQLSOl"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n",
        "Splitting the dataset into training and test sets and start working with the models"
      ],
      "metadata": {
        "id": "JFfG2qWZdoZi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
      ],
      "metadata": {
        "id": "0y7qn1h0Lg4_"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vectorizer\n",
        "Using the CountVectorizer with each bigram to create the number of occurences for the bigram. Then we can create the pipeline that vectorizes our data and gives it to the naive bayes model"
      ],
      "metadata": {
        "id": "tW8csJ85r8wm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnt = CountVectorizer(analyzer = 'char',ngram_range=(2,2))\n",
        "\n",
        "pipeline = Pipeline([\n",
        "   ('vectorizer',cnt),  \n",
        "   ('model',MultinomialNB())\n",
        "])"
      ],
      "metadata": {
        "id": "uBnUqQ-HLi-G"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fitting the pipeline and calculating predictions on the test"
      ],
      "metadata": {
        "id": "UwJqtqkts4pi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline.fit(X_train,y_train)\n",
        "y_pred = pipeline.predict(X_test)"
      ],
      "metadata": {
        "id": "AxYh9e7iLlIJ"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# confusion_matrix(y_test, y_pred)"
      ],
      "metadata": {
        "id": "x7Et7VKWYLiz"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "XA-poKX2YZ2r"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating a dictionary with the language codes as keys and the language names as values to give a more friendly output on predictions"
      ],
      "metadata": {
        "id": "K-QD6c3ctUQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "languages = {\"en\":\"English\", \"it\":\"Italian\", \"de\":\"German\", \"pt\":\"Portuguese\"}"
      ],
      "metadata": {
        "id": "qwEpeUONwCbJ"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tests\n",
        "A test prediction with a sentence in English. Expected outcome is 'en', which is passed into the languages dictionary for an output of \"English\""
      ],
      "metadata": {
        "id": "PV_d-kREtydl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lang = pipeline.predict([\"\"\"Braverman lived in France for two years, as an Erasmus \n",
        "Programme student and then as an Entente Cordiale Scholar, where she completed \n",
        "a master's degree in European and French law at Panthéon-Sorbonne University. \"\"\"])\n",
        "\n",
        "print(\"This text is in\", languages[lang[0]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCntCAbFZTfE",
        "outputId": "84d84bbc-3923-4ae3-ab8e-d0e915728145"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This text is in English\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A test prediction with a sentence in German. Expected outcome is 'de'"
      ],
      "metadata": {
        "id": "r5oiu-FJuMAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lang = pipeline.predict([\"Braverman lebte zwei Jahre in Frankreich, als Studentin im Erasmus-Programm und dann als Stipendiatin der Entente Cordiale, wo sie einen Master-Abschluss in europäischem und französischem Recht an der Universität Panthéon-Sorbonne absolvierte\"])\n",
        "print(\"This text is in\", languages[lang[0]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GhiAfgQ8aT-A",
        "outputId": "2888cd6d-1665-4eda-838b-1aa948d29888"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This text is in German\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A test prediction with a sentence in Portuguese. Expected outcome is 'pt'"
      ],
      "metadata": {
        "id": "FJAY8RWQuUeC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lang = pipeline.predict([\"Braverman morou na França por dois anos, como aluna do Programa Erasmus e depois como Entente Cordiale Scholar, onde completou um mestrado em direito europeu e francês na Universidade Panthéon-Sorbonne\"])\n",
        "print(\"This text is in\", languages[lang[0]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1212fff6-56dc-4df2-a9d7-4268bab39fd5",
        "id": "wf90jdLn4xTk"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This text is in Portuguese\n"
          ]
        }
      ]
    }
  ]
}